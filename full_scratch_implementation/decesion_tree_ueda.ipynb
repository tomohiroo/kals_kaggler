{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 決定木のアルゴリズム"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='desitiontree.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目的関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ジニ不純度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "I_G(t) = 1 - \\sum_{i=1}^c p(i|t)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これを目的関数にとる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これが大きいほどそのノード内にいろんなものがごちゃごちゃしていて、小さいほどそのノード内が整理されていることを示す。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 情報利得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "IG(D_p, f) = I_G(D_p) - \\frac{N_{left}}{N_p} I_G(D_{left}) - \\frac{N_{right}}{N_p} I_G(D_{right}) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "親ノード D_p => 子ノード D_{left}, D_{right} と分ける\\\\\n",
    "親ノード内のトレーニングサンプルの数: N_p\\\\\n",
    "子ノード内のトレーニングサンプルの数をそれぞれN_{left}、N_{right}\\\\\n",
    "分割する特徴量: f\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "つまり　**親の不純度 - 子の不純度** ってこと！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これが大きい質問が**良い質問**、ということになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "過学習を起こしやすいので、ノードの深さの上限を指定したり、ノード分けでだんだん小さくなっていく (情報利得) × (ノード内のデータ数割合) に閾値を設けたりする。\n",
    "\n",
    "また、他のモデルとして、ランダムフォレストや勾配ブースティング木などのアンサンブルも存在する（今度できたら？）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 説明変数の重要度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "決定木がよく使われる理由の一つに説明変数の重要度を出力できる、と言うことがある。\n",
    "\n",
    "情報利得が最大になるようにで分割するので、全情報利得のうちの、その説明変数が占める割合で表される。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "FI(F_j) = \\sum_{t \\in N_{f_j}}IG(t,j_j)n_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "j番目の特徴量f_jで分割されたノードの集合:n_{f_j}\\\\\n",
    "N_{f_j}に含まれるノードtのサンプル数: n_t\\\\\n",
    "f_jの重要度：FI(f_j)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正規化された特徴量は、\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "FI_n(f_j) = \\frac{FI_(f_j)}{\\sum_{j=1}^{n} FI(f_j)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "と表される"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 具体的なアルゴリズム"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 特徴量の１つに注目し、その値を昇順に並べ、各隣り合う値の中間値で分けて行き、情報利得を参照していく\n",
    "2. その中で情報利得が最大になる分け方を選択(もし同じになればランダム)\n",
    "3. これを繰り返していく"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方針"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "機能は以下の通り"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. fit(data, target)で学習\n",
    "2. predict(data)で予測\n",
    "3. score(data, target)で正答率\n",
    "4. feature\\_importances\\_で重要度を出す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再帰関数で書いていく"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=1px>（オブジェクト指向っぽくかけたらいいな。）</font>\n",
    "\n",
    "_NodeクラスとDecisionTreeクラスと_TreeAnalysiusの3つのクラスからなる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "決定木自体の過学習回避には、(情報利得) × (ノード内のデータ数割合)　の閾値設定とmax_depthの二つがあるが、今回は両方実装する!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Node:\n",
    "        #data, target は np.array\n",
    "        def __init__(self, data, max_depth, random_state, standard):\n",
    "            self.right = None\n",
    "            self.left = None\n",
    "            self.max_depth = max_depth\n",
    "            self.random_state = random_state\n",
    "            self.n_features = data.shape[1]\n",
    "            self.label = None\n",
    "            self.standard = None\n",
    "            self.best_feature_idx = None\n",
    "            self.best_threshold = None\n",
    "        # 子ノード(left, right)に分ける\n",
    "        def split(self, data, target, depth):\n",
    "            self.depth = depth\n",
    "            \n",
    "            #自ノードの分析\n",
    "            class_cnt = {i: len(target[target==i]) for i in np.unique(target)}\n",
    "            self.label = max((value, key) for (key, value) in class_cnt.items())[1]\n",
    "            \n",
    "            self.label = target[0]\n",
    "            if depth == self.max_depth:\n",
    "                return\n",
    "            self.gini_idx = self.gini(target)\n",
    "            \n",
    "            #各情報利得を計算し、最大のものを記憶\n",
    "            self.best_info_gain = 0\n",
    "            \n",
    "            if self.random_state is not None:\n",
    "                np.random.seed(self.random_state)\n",
    "            loop_order = np.random.permutation(self.n_features)\n",
    "            for i in loop_order:\n",
    "                unique_features = np.unique(data[:, i])\n",
    "                self.thresholds = (unique_features[:-1] + unique_features[1:]) / 2\n",
    "                for threshold in self.thresholds:\n",
    "                    child_left = target[data[:, i] < threshold]\n",
    "                    child_right = target[data[:, i] >= threshold]\n",
    "                    info_gain = self.info_gain(child_left, child_right, target)\n",
    "                    \n",
    "                    if info_gain > self.best_info_gain:\n",
    "                        self.best_info_gain = info_gain\n",
    "                        self.best_feature_idx = i\n",
    "                        self.best_threshold = threshold\n",
    "                        self.child_left = child_left\n",
    "                        self.child_right = child_right\n",
    "                        \n",
    "            if not(self.best_info_gain):\n",
    "                return\n",
    "            \n",
    "            # 再帰させる\n",
    "            data_left = data[data[:, self.best_feature_idx] < self.best_threshold]\n",
    "            data_right = data[data[:, self.best_feature_idx] >= self.best_threshold]\n",
    "            \n",
    "            target_left = target[data[:, self.best_feature_idx] < self.best_threshold]\n",
    "            target_right = target[data[:, self.best_feature_idx] >= self.best_threshold]\n",
    "            \n",
    "            self.left = _Node(data_left, self.max_depth, self.random_state, self.standard)\n",
    "            self.right = _Node(data_right, self.max_depth, self.random_state, self.standard)\n",
    "            \n",
    "            self.left.split(data_left, target_left, self.depth+1)\n",
    "            self.right.split(data_right, target_right, self.depth+1)\n",
    "            \n",
    "        def predict(self, data):\n",
    "            if not(self.best_info_gain):\n",
    "                return self.label\n",
    "            if data[self.best_feature_idx] < self.best_threshold:\n",
    "                return self.left.predict(data)\n",
    "            return self.right.predict(data)\n",
    "            \n",
    "        def gini(self, target):\n",
    "            sigma = 0\n",
    "            for i in np.unique(target):\n",
    "                sigma += (len(target[target==i]) / len(target)) ** 2\n",
    "            return (1 - sigma)\n",
    "\n",
    "        def info_gain(self, left, right, target):\n",
    "            return (self.gini_idx - len(left) / len(target) * self.gini(left) - len(right) / len(target) * self.gini(right))\n",
    "\n",
    "        def trim(self, standard):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTree:\n",
    "    def __init__(self, max_depth=None, random_state=None, standard=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.random_state = random_state\n",
    "        self.standard = standard\n",
    "        \n",
    "    def fit(self, data, target):\n",
    "        self.root = _Node(data, self.max_depth, self.random_state, self.standard)\n",
    "        self.root.split(data, target, 0)\n",
    "        self.root.trim(self.standard)\n",
    "        self.feature_importances = None\n",
    "        \n",
    "    def  predict(self, data):\n",
    "        ans = []\n",
    "        if len(data.shape) == 1:\n",
    "            return self.root.predict(data)\n",
    "        for d in data:\n",
    "            ans.append(self.root.predict(d))\n",
    "        return np.array(ans)\n",
    "    \n",
    "    def score(self, data, target):\n",
    "        pred = self.predict(data)\n",
    "        return(len(pred[pred==target]) / len(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実際に使ってみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearnのDecisionTreeClassifierと比較する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## アヤメ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdt = MyDecisionTree(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "print(mdt.score(X_train,y_train))\n",
    "print(mdt.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "print(dtc.score(X_train,y_train))\n",
    "print(dtc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一緒！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 課題、感想等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そもそも仕組みが単純なモデルであること、有能な参考文献が手に入ったことで実装することができた。\n",
    "\n",
    "実際はジニ係数の他に、引数によって情報エントロピーも取れるので、その実装もできればよかった。\n",
    "\n",
    "ランダムフォレストの実装もできたらいいな、、、と思う。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
