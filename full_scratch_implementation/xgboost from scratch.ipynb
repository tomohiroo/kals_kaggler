{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "def logistic_loss(y, t):\n",
    "    loss = - (t * np.log(y) + (1 - t) * np.log(1 - y))\n",
    "    return loss\n",
    "\n",
    "def least_square(y, t):\n",
    "    loss = np.linalg.norm(y -t)\n",
    "    return loss\n",
    "\n",
    "\n",
    "class Objective(object):\n",
    "    \"\"\"\n",
    "    目的関数のabstract class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, activate):\n",
    "        self.activate = activate\n",
    "\n",
    "    def __call__(self, y, t):\n",
    "        raise NotImplementedError(\"Objecttive must implement `__call__` function\")\n",
    "\n",
    "\n",
    "class CrossEntropy(Objective):\n",
    "    \"\"\"\n",
    "    活性化関数:シグモイド関数\n",
    "    ロス関数:交差エントロピー\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(activate=sigmoid)\n",
    "        return\n",
    "\n",
    "    def __call__(self, y, t):\n",
    "        pred = self.activate(y)\n",
    "        grad = pred - t\n",
    "        hess = pred * (1.0 - pred)\n",
    "        return grad, hess\n",
    "\n",
    "\n",
    "class LeastSquare(Objective):\n",
    "    \"\"\"\n",
    "    二乗ロス関数\n",
    "    * 活性化関数: f(x) = x\n",
    "    * 目的関数: || x - t || ^ 2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # 二乗ロス関数の時活性化関数は恒等写像\n",
    "        super().__init__(lambda x: x)\n",
    "\n",
    "    def __call__(self, y, t):\n",
    "        pred = self.activate(y)\n",
    "        grad = 2. * (pred - t)\n",
    "        hess = 2. * np.ones_like(pred)\n",
    "        return grad, hess\n",
    "\n",
    "\"\"\"\n",
    "utility functions\n",
    "\"\"\"\n",
    "\n",
    "__author__ = \"nyk510\"\n",
    "\n",
    "from logging import getLogger, StreamHandler, Formatter\n",
    "\n",
    "def get_logger(name, level=\"DEBUG\"):\n",
    "    logger = getLogger(name)\n",
    "    sh = StreamHandler()\n",
    "    fmter = Formatter('{asctime}\\t{name}\\t{message}', style='{')\n",
    "    sh.setFormatter(fmter)\n",
    "    sh.setLevel(level)\n",
    "    logger.setLevel(level)\n",
    "    logger.addHandler(sh)\n",
    "    return logger\n",
    "\n",
    "class Node(object):\n",
    "    \"\"\"\n",
    "    Gradient Boostingで作成する木構造のノード objest\n",
    "    \"\"\"\n",
    "\n",
    "    totalNodeNum = 0\n",
    "\n",
    "    def __init__(self, x, t, grad, hess, lam=1e-4, objective_function=\"cross_entropy\"):\n",
    "        \"\"\"\n",
    "        :param x:\n",
    "        :param t:\n",
    "        :param grad:\n",
    "        :param hess:\n",
    "        :param lam:\n",
    "        :param objective_function: 目的関数\n",
    "        \"\"\"\n",
    "\n",
    "        if len(x.shape) == 1:\n",
    "            x = x.reshape(-1, 1)\n",
    "\n",
    "        Node.totalNodeNum += 1\n",
    "        self.id = Node.totalNodeNum\n",
    "        self.x = x\n",
    "        self.t = t\n",
    "        self.grad = grad \n",
    "        self.hess = hess\n",
    "        self.lam = lam\n",
    "\n",
    "        if objective_function == \"cross_entropy\":\n",
    "            self.objective_function = CrossEntropy()\n",
    "        elif isinstance(objective_functioin, Objective):\n",
    "            self.objective_function = objective_function\n",
    "        else:\n",
    "            raise ValueError(\"objective_function must be `Objective` instance\")\n",
    "\n",
    "        self.feature = None\n",
    "        self.threshold = None\n",
    "        self.right = None\n",
    "        self.left = None\n",
    "        self.has_children = False\n",
    "        self.already_calculated_gain = False\n",
    "        self.num_feature = x.shape[1]\n",
    "        self.num_data = x.shape[0]\n",
    "\n",
    "        # predict values clustered in this node.\n",
    "        self.y = - grad.sum() / (lam + hess.sum())\n",
    "\n",
    "        self.loss = self.calculate_children_objective()\n",
    "\n",
    "        self.best_gain = 0.\n",
    "        self.best_threshold = None\n",
    "        self.best_feature_idx = None\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        :param np.ndarray x:\n",
    "        :return: np.ndarray\n",
    "        \"\"\"\n",
    "        if self.has_children:\n",
    "            return np.where(x[:, self.feature] < self.threshold,\n",
    "            self.left.predict(x),\n",
    "            self.right.predict(x))\n",
    "        else:\n",
    "            return self.y\n",
    "    \n",
    "    def _calculate_objective_value(self, grad, hess):\n",
    "        \"\"\"\n",
    "        勾配、へシアン情報から、二次近似された objective functionの値を計算\n",
    "        :param np.ndarray grad\n",
    "            目的関数の勾配の配列 shape = (n_samples, )\n",
    "        :rtype float\n",
    "        \"\"\"\n",
    "\n",
    "        obj_val = - grad.sum() ** 2. / (self.lam + hess.sum()) / 2.\n",
    "        return obj_val\n",
    "\n",
    "    def calculate_index_obj(self, idx):\n",
    "        \"\"\"\n",
    "        自分の持っているデータの中の一部を使って`objective function`の値を計算\n",
    "        :param np.ndarray idx: 求めたいデータのindexの配列. shape = (n_samples, )\n",
    "        :rtype objective value\n",
    "        :rtype float\n",
    "        \"\"\"\n",
    "        return self._calculate_objective_value(self.grad[idx], self.hess[idx])\n",
    "\n",
    "    def build(self, best_gain):\n",
    "        \"\"\"\n",
    "        best_gainと同じ値をもつノードの分割を実行する\n",
    "\n",
    "        子ノードが存在する場合は、子ノードどちらかに`best_gain`を持つものが存在するので\n",
    "        + `best_gain`を持つものがあるかどうかチェック\n",
    "        +子ノードに対し再帰的に`build`の呼び出し\n",
    "        \"\"\"\n",
    "\n",
    "        if self.has_children:\n",
    "            if self.left.best_gain > self.right.best_gain:\n",
    "                self.left.build(best_gain)\n",
    "            else:\n",
    "                self.right.build(best_gain)\n",
    "\n",
    "        else:\n",
    "            self.feature = f_idx = self.best_feature_idx\n",
    "            self.threshold = threshold = self.best_threshold\n",
    "            x = self.x\n",
    "            t = self.t\n",
    "\n",
    "            left_idx = x[:, f_idx] < threshold\n",
    "            right_idx = x[:, f_idx] >= threshold\n",
    "\n",
    "            logger.debug('left-count:{0}\\tright-count:{1}\\tfeature_index:{2}'.format(\n",
    "                sum(left_idx), sum(right_idx), f_idx))\n",
    "\n",
    "            l_x, l_t, l_g, l_h = x[left_idx], t[left_idx], self.grad[left_idx], self.hess[left_idx]\n",
    "            r_x, r_t, r_g, r_h = x[right_idx], t[right_idx], self.grad[right_idx], self.hess[right_idx]\n",
    "\n",
    "            self.left = Node(x=l_x, t=l_t, grad=l_g, hess=l_h)\n",
    "            self.right = Node(x=r_x, t=r_g, grad=r_g, hess=r_h)\n",
    "            self.has_children = True\n",
    "            self.already_calculated_gain = False\n",
    "            return \n",
    "\n",
    "    def calculate_best_gain(self):\n",
    "        \"\"\"\n",
    "        自分以下のノードが分割された時の最も良い`gain`の値を計算しそれを返す\n",
    "        末端のノードの際にはそれに加えて以下を保存する\n",
    "        + どの`index`で分割を行うか - `best_feature_idx`\n",
    "        + 閾値を幾つで分割するか - `best_threshold`\n",
    "        \"\"\"\n",
    "\n",
    "        # 親ノードのとき子ノードに計算を再帰的によびだし\n",
    "        if self.has_children:\n",
    "            l = self.left.calculate_best_gain()\n",
    "            r = self.right.calculate_best_gain()\n",
    "            self.best_gain = max(l, r)\n",
    "            return self.best_gain\n",
    "\n",
    "        # 以下は全て末端ノード\n",
    "        # 計算済みであればそれを返す\n",
    "        if self.already_calculated_gain:\n",
    "            return self.best_gain\n",
    "\n",
    "        # 以下は計算していない末端ノードに対する計算になる\n",
    "        # 自分に属するデータが１つしかないときこれ以上分割できないので終了\n",
    "        if self.num_data <= 1:\n",
    "            return self.best_gain\n",
    "\n",
    "        # 全ての特徴量で分割の最適化を行ってもっとも良い分割を探索\n",
    "        for f_idx in range(self.num_feature):\n",
    "            logger.debug(f_idx)\n",
    "            data_f = np.unique(self.x[:, f_idx])\n",
    "            sep_points = (data_f[1:] + data_f[:-1]) / 2.\n",
    "\n",
    "            for threshold in sep_points:\n",
    "                left_idx = self.x[:, f_idx] < threshold\n",
    "                right_idx = self.x[:, f_idx] >= threshold\n",
    "                loss_left = self.calculate_index_obj(idx=left_idx)\n",
    "                loss_right = self.calculate_index_obj(idx=right_idx)\n",
    "                gain = self.loss - loss_left - loss_right\n",
    "\n",
    "                # すでに計算されているもっとも大きなゲインより大きい場合には更新する\n",
    "                if self.best_gain < gain:\n",
    "                    self.best_gain = gain\n",
    "                    self.best_threshold = threshold\n",
    "                    self.best_feature_idx = f_idx\n",
    "\n",
    "        #一度計算したら再度分割されるまでは同じなので,already_calculated_gain = trueとする\n",
    "        self.already_calculated_gain = True\n",
    "\n",
    "        return self.best_gain\n",
    "\n",
    "    def calculate_children_objective(self):\n",
    "        \"\"\"\n",
    "        自分の持っているノード全ての目的関数を計算する\n",
    "        :return: 目的関数値\n",
    "        :rtype float\n",
    "        \"\"\"\n",
    "        if self.has_children:\n",
    "            return self.left.calculate_children_objective() + self.right.calculate_children_objective()\n",
    "\n",
    "        # 末端のーどのとき真面目に計算\n",
    "        loss = self._calsulate_objective_value(grad=self.grad, hess=self.hess)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class GradientBoostedDT(object):\n",
    "    \"\"\"\n",
    "    Gradient Boosted Decision Treeによる予測モデル\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, objective=\"cross_entropy\", loss=\"logistic\",max_depth=8, gamma=1., num_iter=20, eta=.1, lam=.01):\n",
    "        \"\"\"\n",
    "        :param str | Objective objective:\n",
    "        回帰する目的関数 or それを表す文字列\n",
    "        要するにcallした時に(grad, hess)のtupleを返す必要がある\n",
    "        :param str| () loss ロス関数 `logistic` or callable object\n",
    "        :param int max_depth: 分割の最大値\n",
    "        :param float gamma:\n",
    "        木を１つ成長させることに対するペナルティ\n",
    "        gainの値がgammaを超えない場合きの分割をstopする\n",
    "        :param int num_iter: boostingを繰り返す回数\n",
    "        :param float eta: boostingのステップサイズ\n",
    "        :param float lam: 目的関数の正則化パラメータ\n",
    "        \"\"\"\n",
    "        if objective == \"cross_entropy\":\n",
    "            self.objective = CrossEntropy()\n",
    "        elif isinstance(objective, Objective):\n",
    "            self.objective = Objective\n",
    "        else:\n",
    "            logger.warning(type(objective))\n",
    "            raise ValueError(\"Only support `cross_entropy`. Actually: {}\".format(objective))\n",
    "\n",
    "        if loss == \"logistic\":\n",
    "            self.loss = logistic_loss\n",
    "        elif hasattr(loss, \"__call__\"):\n",
    "                self.loss = loss\n",
    "        else:\n",
    "            raise ValueError(\"arg `loss` is only supported `logistic`. \")\n",
    "\n",
    "        self.activate = self.objective.activate\n",
    "        self.trees = []\n",
    "        self.max_depth = max_depth\n",
    "        self.gamma = gamma\n",
    "        self.num_iter = num_iter\n",
    "        self.eta = enumerate\n",
    "        self.lam = lam\n",
    "        self.training_loss = None\n",
    "        self.validation_loss = None\n",
    "        self.f = None\n",
    "\n",
    "    def fit(self, x, t, valid_data=None, vervose=1):\n",
    "        \"\"\"\n",
    "        :param np.ndarray x: 特徴量のnumpy array shape = (n_samples,)\n",
    "        :param np.ndarray t:目的変数のnumpy array shape = (n_samples, )            :param [np.ndarray, np.ndarray] | None valid_data:\n",
    "        (x, t) で構成された validation data.            None 以外が与えら得た時各 iteration ごとにこのデータを用いて validation loss を計算する.\n",
    "        :param int verbose:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if verbose > 0:\n",
    "            logger.setLevel(\"INFO\")\n",
    "        elif verbose > 1:\n",
    "            logger.setLevel(\"DEBUG\")\n",
    "        if len(x.shape) == 1:\n",
    "            x = x.reshape(-1, 1)\n",
    "        self.f = np.zeros_like(t)\n",
    "        self.training_loss = []\n",
    "        if valid_data is not None:\n",
    "            self.validation_loss = []\n",
    "\n",
    "        for i in range(self.num_iter):\n",
    "            # 直前の予測値と目的の値とで勾配とへシアンを計算\n",
    "            grad, hess = self(self.f, t)\n",
    "\n",
    "            root_node = Node(x=x, t=t, grad=grad, hess=hess)\n",
    "\n",
    "            for depth in range(self.max_depth):\n",
    "                logger.debug('object_value:\\t{0:.2f}'.format(root_node.calculate_children_objective()))\n",
    "                logger.debug('iterate:\\t{0},\\tdepth:\\t{1}'.format(i, depth))\n",
    "\n",
    "                best_gain = root_node.calculate_best_gain()\n",
    "                logger.debug('Best Gain:\\t{0:.2f}'.format(best_gain))\n",
    "\n",
    "                if best_gain < self.gamma:\n",
    "                    break\n",
    "                else:\n",
    "                    root_node.build(best_gain=best_gain)\n",
    "                \n",
    "            self.trees.append(root_node)\n",
    "            f_i = root_node.predict(x)\n",
    "            self.f += self.eta * f_idx\n",
    "            train_loss = self._current_train_loss(t)\n",
    "            logger.info('iterate:{0}\\tloss:{1:.2f}'.format(i, train_loss))\n",
    "            self.training_loss.append(train_loss)\n",
    "\n",
    "            if valid_data is not None:\n",
    "                valid_x, valid_t = valid_data\n",
    "                pred = self.predict(valid_x)\n",
    "                pred_loss = self.loss(pred, valid_t).sum()\n",
    "                self.validation_loss.append(pred_loss)\n",
    "                logger.info('testloss:\\t{0:.2f}'.format(pred_loss))\n",
    "        return self\n",
    "    def _current_train_loss(self, t):\n",
    "        \"\"\"\n",
    "        学習途中でのロス関数の値を計算する\n",
    "        :param np.ndarray t: target values\n",
    "        :return: loss values\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        a = self.activate(self.f)\n",
    "        loss = self.loss(a, t)\n",
    "        return loss.sum()\n",
    "\n",
    "    def predict(self, x, use_trees=None):\n",
    "        \"\"\"\n",
    "        :param np.ndarray x:\n",
    "        :param None | int use_trees:\n",
    "            予測に用いる木の数. 存在している木の数よりも大きい時や\n",
    "            負の値が設定された時はすべての木を用いて予測する\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if use_trees and 1 <= use_trees <= len(self.trees):\n",
    "            nodes = self.trees[:use_trees]\n",
    "        else:\n",
    "            nodes = self.trees\n",
    "\n",
    "        a = np.zeros_like(x[:, 0])\n",
    "        for i , tree in enumerate(nodes):\n",
    "            a += self.eta * tree.predict(x)\n",
    "        pred = self.activate(a)\n",
    "        return pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosted Tree (Xgboost) の取り扱い説明書\n",
    "https://qiita.com/nyk510/items/7922a8a3c1a7b622b935\n",
    "http://kefism.hatenablog.com/entry/2017/06/11/182959\n",
    "実装\n",
    "https://github.com/nyk510/gradient-boosted-decision-tree/blob/master/gbdtree/gbdtree.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
